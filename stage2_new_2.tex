\documentclass[10pt]{article}
\usepackage[usenames]{color} %used for font color
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage[utf8]{inputenc} %useful to type directly diacritic characters

\usepackage{graphics}
\usepackage{adjustbox}
\usepackage{float}
\begin{document}
\begin{raggedright}
Michael Lan \\
Lingfei Zeng \\
Xunjie Zhu \\
\end{raggedright}

\vspace{5mm} 

Stage2 - The Design Stage. 
\begin{itemize} 
\item{ Design Description:\\} 
\begin{itemize}
There are three types of users who will use this system. We will begin with the first user type: 1) users who select a topic and bring up a cluster of tweets related to that topic.

\item[$\diamond$]{1) Tweet clustering based on semantic similarity}

We aim to cluster tweets into different groups using Hierarchical Clustering. To do this, we require semantic similarity scores. We compute these using the extended {\bf Edit Distance} algorithm integrated {\bf Word2Vec} embedding to compute the distance between sentences in the tweets, and by grouping by shortest distance we divide tweets into different groups. \\\\
In details, we describe tweet sentence $S_1$ as $S_1$=\{$a_1$,$a_2$,…,$a_n$\} and sentence $S_2$ as $S_2$ =\{$b_1$,$b_2$, …,$b_m$\}. $S_1$ consists of n words and $S_2$ consists of m words. $a_i$ is the i th word of $S_1$ and $b_j$ is the j th word of $S_2$. We use Sim($S_1$,$S_2$) to describe the similarity between $S_1$ and $S_2$, with the range of 0 (no relation) to 1 (semantic equivalence). For edit distance, we utilize Levenshtein Distance algorithm to extend to the word sets in sentences. We define the Levenshtein distance between $S_1$ and $S_2$ is L(n,m), where  \(0\leq i\leq n\) and \(0\leq j\leq n\). And we have: \[L(i, j) = max(i, j)  \qquad   if \quad min(i,j)=0, \]  

$$
L(i, j) = min
\left\{
\begin{array}{ll}
L(i-1, j-1) + c(a_i,b_j) \\
L(i, j-1) +1 \\
L(i-1, j) +1
\end{array}
\right.
otherwise.
$$
Where the function c($a_i$,$b_j$) is defined as
$$
c(a_i, b_j)=
\left\{
\begin{array}{ll}
0 \quad (a_i = b_j) \\
1 \quad (a_i \neq b_j)
\end{array}
\right.
$$

We also need to consider the circumstances that if two different words have the same or similar meaning (e.g., “cat” and “kitty”), in which case edit distance defines them as a mismatch. We address this issue by introducing Word2vec to measure semantic similarity between words. 
%{\sf Word2vec}~\cite{word2vec} 
Word2vec [1] is an unsupervised  deep learning algorithm that maps each word to a vector  $\in \mathcal{R}^n$ such that the semantically similar words are mapped to the vectors that are close to each other in the geometry space. 
In our settings, given any two words, e.g.,  $a_i$ and $b_j$ in the above, we first apply word2vec to map them to two vectors, $\vec{v}_1$ and $\vec{v}_2$, and then use the euclidean distance, $\lVert \vec{v}_1 - \vec{v}_2 \rVert$, to measure the cost of the replacement.
We may normalize the cost into a relatively small range. 

$$
\begin{array}{c}
c(a_i, b_j)=\lVert \vec{v}_1 - \vec{v}_2 \rVert \\
 where \quad  \vec{v}_1 = word2vec(a_i), \vec{v}_2=word2vec(b_j)
\end{array}
$$

\item[$\diamond$]{2a) Recommending similar tweets and hashtags based on structural similarity}\\

Next, we will describe the other 2 user types: 2) users, searching for a hashtag H, who want to find tweets related to H, but do not contain H and 3) users who want to find hashtags similar to a tweet that does not contain those hashtags. In case 2), the Tweet Search job algorithm will be invoked. In case 3), the Hashtag Search job algorithm will be invoked. \\

Before any search jobs are invoked, a directed bipartite network G with two different types of nodes, tweets and hashtags, will be created. Instead of storing them as strings, we give a unique ID to each tweet and hashtag that allows for efficient identification. A tweet will have an outgoing edge to a hashtag if it contains it. The graph $ G^2$ has as its nodes every pair of tweets and every pair of hashtags in G. In Algorithm 3, we apply SimRank on $ G^2$  to find a similarity score for every pair of tweets. The more outgoing edges two tweets have to the same hashtags, the more similar they are. If this network does not contain the tweet or the hashtag that the user inputs, then the search algorithms will return that no results were found. \\

The structural similarity, based on SimRank, is evaluated using these two equations, which will be explained below:
Let A and B be tweets. O(v) are the outgoing neighbors of v. The similarity s(A,B) is:

$$s(A,B) = \dfrac{C\textsubscript{1}}{|O(A)||O(B)|} \sum_{i=1}^{|O(A)|} \sum_{j=1}^{|O(B)|} s(O\textsubscript{i}(A),O\textsubscript{j}(B))$$

C is a constant between 0 and 1. It is the decay factor and is the only adjustable parameter of the algorithm. The reason for using C will be explained later. \\

Let c and be be hashtags. I(v) are the ingoing neighbors of v.  The similarity s(c,d) is:

$$s(c,d) = \dfrac{C\textsubscript{2}}{|I(c)||I(d)|} \sum_{i=1}^{|I(c)|} \sum_{j=1}^{|I(d)|} s(I\textsubscript{i}(c),I\textsubscript{j}(d))$$

The similarity between two tweets is the average similarity of their hashtags, O(A) and O(B). The similarity between two hashtags is the average similarity of tweets that contain them. Let A be either a hashtag or a tweet. The starting scores for the algorithm are the nodes (A,A), which are given similarity score of 1, while all other nodes are given a similarity score of 0. In the next step, the similarity scores for hashtag nodes with an ingoing edges from tweet nodes (A,A) and tweet nodes with outgoing edges to hashtag nodes (x,x) are computed. These scores are calculated using C*s(A,A), because two nodes that share a node by 1 degree of similarity as less similar than two nodes which are the same tweet. This iteration occurs until step K, such that the scores from step K-1 are the same as the scores at step K. \\

Similarity scores were shown to always exist, are unique, and are symmetric for any two nodes. Since s(a,b) and s(b,a) are symmetric, so only need to store one of their scores. Algorithm 4, the tweet search job, finds node pairs, which contain the searched tweet, that are most similar to the searched tweet. Then, algorithm finds the hashtags contained in the input tweet, and stores them in a set A. It also finds the hashtags contained in these similar tweets, and stores them in a set B. Finally, it returns the relative complement B-A, the desired output. Algorithm 5, the hashtag search job, works in the same way, but switches the roles of tweets and hashtags. \\

Many of the hashtags that are returned will too general to characterize specific topics. For instance, many tweets may contain the hashtag \#news, which may often be 2 degree of separations from \#sports and \#fashion. At first, this might imply that (\#sports, \#fashion) have high similarity because there are many (not directed) paths of length 4 connecting them, meaning they're often separated by 4 degrees. But their similarity is not only determined by the number of connections, but the score of those connected nodes. A tweet with both\#sports and \#news, and a tweet with both \#news and \#fashion, may have \#news in common, but not much else. These means these tweets are often not very similar to each other, so they are weighted very lightly when determining the score of (\#sports, \#fashion). Thus SimRank takes these very general hashtags into account. It uses the idea of mutual reinforcement to find which hashtags can specifically characterize tweet topics. \\

This algorithm does not aim to find phrases which distinguish one topic from another; rather, it seeks to bridge communities by finding their similarities. \\

\item[$\diamond$]{2b)Time Complexity of Algorithm 3 to 5}\\

Assume we have constructed the graph $ G^2$  by using graph G. Now let d = the average of $|I(a)|*|I(b)|$ over all nodes in $ G^2$ . Let K = average number of iterations the algorithm performs. Let n be the number of nodes in $ G^2$ . Using a ‘naive’ implementation of SimRank, average running time of Algorithm 3 is O(K*n*d). Its worst case mainly depends on K. \\

Algorithms 4 and 5 are essentially the same, but switch the roles of tweets and hashtags. P = the number of tweet (for Algorithm 5) or hashtag (for Algorithm 5) nodes of $ G^2$ . WLOG, we will analyze Algorithm 4 and say Algorithms 4 and 5 have the same running time. First, it finds all the node pairs containing the input, which takes O(P); if the input is not in the graph, it returns “No results were found.” \\

Let R be the length of the node scores hash table. The algorithm sorts the scores, and Mergesort has an average run time of O(R*log(R)). Next, it finds all the neighbors of the input. We do not have an average case, so we look at the worst case, which is O(n-P). \\

Most of the running time is due to computing the relative complement. Assuming the set contains no duplicate elements, this step compares two sets, so it takes O(P*(P-1)) * (the comparison cost); the comparison cost is constant because we are not comparing lengthy strings, but IDs. The total running time is:
O(P + R*log(R) + (n-P) + P*(P-1)) = O(P*(P-1)), which is polynomial.  \\

\item[$\diamond$]{2c)Space Complexity of Algorithm 3 to 5}\\

In Algorithm 3, we store node\_scores in a hash table, so its space complexity  is O(n). In Algorithm 4, we have two hash tables of hashtags, which each cost O(n-P) to store. We have one hash table of tweets, which costs O(P) to store. So the total space complexity of Algorithm 4 is O(n-P + P) = O(n). WLOG, the total space complexity of Algorithm 5 is O(n). \\\\\\\\

\end{itemize}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{C:/Users/Michael/Downloads/user_flowchart.png}
\end{figure}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{C:/Users/Michael/Downloads/cluster_flowchart.png}
\end{figure}

\pagebreak

\item{ High Level Pseudo Code: refer to Algorithms 1 to 5 }

\begin{algorithm}[H]
	\SetKwData{Left}{left}
	\SetKwData{This}{this}
	\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}
	\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\Input{A sentence $S_1$ with n words as $S_1$=\{$a_1$,$a_2$,…,$a_n$\} and a sentence $S_2$ with m words $S_2$ =\{$b_1$,$b_2$, …,$b_m$\} }
	\Output{The edit distance between $S_1$ and $S_2$ L(n,m)}
	\BlankLine
	\emph{}\
	\For{$i = 0$ to n}{
		L($i, 0$) = $i$	
	}
    \For{$j = 0$ to m}{
	    L($0, j$) = $j$	
    }

    \For{$i = 1$ to n}{
    	\For{$j = 1 $to m} {
    			L($i, j$) = min \{L($i-1, j$) +1, L($i, j-1$) +1, L($i-1, j-1$) + c($a_i, b_j$)\} 
    		}
    } 
    return L(m, n)	
	
	\caption{Edit Distance}\label{algo_disjdecomp}
\end{algorithm}

\begin{algorithm}[H]
	\SetKwData{Left}{left}
	\SetKwData{This}{this}
	\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}
	\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\Input{An integer n represents the number of initial clusters, a clusterMatrix stores the sentence vector of every cluster, an array, a queue(implemented by binary heap) stores indices and distances between two cluster}
		\Output{clustered nodes}
	    \For{$i = 1$ to n}{
	    	initiaDistance($i, i$) = 0\;
	    	\For{$j = i+1 $to n} {
			initiaDistance($i, j$) = distance($i, j$)
    			queue add entry(initiaDistance($i, j$), i, j, 1, 1)
		}
    } 
      \While{$n \geq miniumClusters$} {
      	entry = queue.pop()\;
	merge(entry.cluster1, entry.cluster2, entry.distance, entry.distance, clusterMatrix)\;
		    \For{i = 1 to n}{\
		    \If{$i \neq entry.cluster1 and clusterMatrix[i].size \neq 0 $}{
		    $first = min(entry.cluster1, i)$\;
		    $second = min(entry.cluster2, i)$\;
		    newDistance = getDistance(initialDistance, clusterMatrix(first), clusterMatrix(second))\;
		    queue add entry(newDistance, first, second, clusterMatrix(first).size, clusterMatrix(second).size)
		    }
	}
			    n = n -1;
      }
	\BlankLine
	\emph{}\
		
\caption{Hierarchical Clustering Algorithm}\label{algo_disjdecomp}
\end{algorithm}

\begin{algorithm}[H]
	\SetKwData{Left}{left}
	\SetKwData{This}{this}
	\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}
	\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\Input{A bipartite graph G\_2 of node pairs }
	\Output{A hash table of similarity scores for every node pair in G\_2}
	\BlankLine
	\emph{}\
	Create hash table node\_scores\;
	\For{(a,b) in nodes(G\_2):}{
		\eIf{a==b}
			{Node\_scores[(a,b)] := 1}
			{Node\_scores[(a,b)] := 0}
	}
	stop\_signal := on\;
	\While{stop\_signal == on}{
		stop\_signal := on\;
		\For{(a,b) in nodes(G\_2)}{
			\If{a != b}
				{\{COMMENT: Let N(v) denote the neighbors of node v \} \\
				R\textsubscript{k}(a,b) := $$\dfrac{C\textsubscript{1}}{|N(a)||N(b)|} \sum_{i=1}^{|N(a)|} \sum_{j=1}^{|N(b)|} R\textsubscript{k-1}(N\textsubscript{i}(a),N\textsubscript{j}(b))$$}
			\If{R\textsubscript{k}(a,b) != R\textsubscript{k-1}(a,b)}
				{stop\_signal := off}
			node\_scores[(a,b)] := R\_k(a,b) 
		}
	}
	Return node\_scores
	\caption{Node pair similarity scores pseudocode}\label{algo_a}
\end{algorithm}

\begin{algorithm}[H]
	\SetKwData{Left}{left}
	\SetKwData{This}{this}
	\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}
	\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\Input{A hashtag H that the user searches for, bipartite graph G\_2, hash table node\_scores}
	\Output{Tweets, without hashtag H, which are similar to tweets with hashtag H}
	\BlankLine
	\emph{}\
	Create hash table related\_hashtags\;
	stop\_signal := on\;
	\For{(a,b) in hashtag\_nodes(G\_2)}{
		\If{a == H}
			{stop\_signal := off\;
			related\_hashtags[b]  := R\_k(a,b)}
	}
	\If{stop\_signal == on}
		{Return “No results were found.”}
	mergesort on related\_hashtags by decreasing value\;
	Similar\_hashtags := 100 elements of related\_hashtags with highest value\;
	Create array H\_tweets\;
	\For{(x,y) in neighbors(H)}{
		\If{x == y and x not in H\_tweets}
			{Add x to H\_tweets}	
	}
	Create array H\_similar\_tweets\;
	\For{hashtag in H\_similar\_hashtag}{
		\For{(x,y) in neighbors(hashtag)}{
			\If{x == y and x not in H\_tweets and x not in similar\_tweets}
				{Add x to H\_similar\_tweets}
		}
	}
	Return H\_similar\_tweets
	\caption{Tweet Search job pseudocode}\label{algo_b}
\end{algorithm}

\begin{algorithm}[H]
	\SetKwData{Left}{left}
	\SetKwData{This}{this}
	\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}
	\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\Input{Hash table node\_scores, input\_tweet, bipartite graph G\_2}
	\Output{Hashtags that not in input\_tweet but are in tweets similar to input\_tweet}
	\BlankLine
	\emph{}\
	Create hash table related\_tweets\;
	stop\_signal := on\;
	\For{(a,b) in tweet\_nodes(G\_2)}{
		\If{a == input\_tweet}
			{stop\_signal := off\;
			related\_tweets[b]  := R\_k(a,b)}
	}
	\If{stop\_signal == on}
		{Return “No results were found.”}
	mergesort on related\_tweets by decreasing value\;
	Similar\_tweets := 100 elements of related\_tweets with highest value\;
	Create array  input\_tweet\_hashtags\;
	\For{(x,y) in neighbors(input\_tweet)}{
		\If{x == y and x not in input\_tweet\_hashtags}
			{Add x to  input\_tweet\_hashtags}	
	}
	Create array similar\_hashtags\;
	\For{tweet in similar\_tweets}{
		\For{(x,y) in neighbors(tweet)}{
			\If{x == y and x not in input\_tweet\_hashtags and x not in similar\_hashtags}
				{Add x to H\_similar\_hashtags}
		}
	}
	Return  similar\_hashtags
	\caption{Hashtag Search job pseudocode}\label{algo_b}
\end{algorithm}


\item{Algorithms and  Data Structures:}
In Algorithm 1, the major algorithm is Edit Distance in Dynamic Programming. The data structure is two-dimensional array, and the time complexity is $ \mathcal{O}(mn)$. In Algorithm 2, we use Agglomerative Hierarchical Clustering Algorithm to cluster the documents. The time complexity of this algorithm is $ \mathcal{O}(n^2logn)$ \\

The core of Algorithm 3 is the SimRank algorithm. The core of Algorithms 4 and 5 is Mergesort. Before computing the graphs, each tweet is stored as a pair of a unique identifier and an array of its hashtags. This pair will be stored as a hash table. The graphs are represented as adjacency lists (array of arrays). Each node is a pair, so it will be stored as a hash table Collections of hashtags and arrays, linked to their similarity scores, are stored in hash tables. Similar tweets and hashtags are stored in sets, a type of array. 
\end{itemize}

\iffalse
\begin{itemize} 
\item{  Flow Diagram Major Constraints.}
Please insert here the integrity constraints:
\begin{itemize} 
\item{ Integrity Constraint. }
Please insert the first integrity constraint in here together with its description and justification. 
\end{itemize}
Please repeat the pattern for each integrity constraint.
\end{itemize}
\fi



%\bibliographystyle{abbrv}
%\bibliography{stage2_new}  % sigproc.bib is the name of the Bibliography in this case

\begin{thebibliography}{2}
%\iffalse
\bibitem{edit}
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed
representations of words and phrases and their compositionality. In Proceed-
ings of the 26th International Conference on Neural Information Processing
Systems, NIPS'13, pages 3111-3119, USA, 2013. Curran Associates Inc.
%\fi
	
\bibitem{simrank} 
Jeh, Glen, and Jennifer Widom. "SimRank: a measure of structural-context similarity." In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 538-543. ACM, 2002.

\end{thebibliography}

\end{document}
